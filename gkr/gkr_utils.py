from zorch import koalabear

# Full-round Poseidon matrix
M = 1 / koalabear.KoalaBear([[1+i+j for i in range(16)] for j in range(16)])
# Partial round Poseidon matrix: J + diag
M_inner = (
    1 + koalabear.KoalaBear([[
        i**5+1 if i==j else 0 for i in range(16)] for j in range(16)
    ])
)
diag_inner = koalabear.KoalaBear([i**5+1 for i in range(16)])
# NOTE: we make a hard assumption elsewhere in the code that these matrices
# are symmetric across the diagonal aka are their own transpose

SHA256_OVERRIDE = False

# We store state in our GKR protocol in 1D. Here, we reshape it so [i,j]
# is the j'th wire of the i'th hash, then multiply it by the matrix, then
# reshape back
def matmul_layer(values, matrix=M):
    orig_shape = values.shape
    size = values.shape[-1] // 16
    values = values.reshape(values.shape[:-1] + (size, 16))
    values = koalabear.matmul(values, matrix)
    values = values.reshape(orig_shape)
    return values

# Same as above, but for partial rounds. Cheaper because the matrix has the
# special form J + diag, so you only need 16 muls and 32 adds (as opposed to
# 256 each for a "full" matrix)
def inner_matmul_layer(values):
    orig_shape = values.shape
    size = values.shape[-1] // 16
    values = values.reshape(values.shape[:-1] + (size, 16))
    values_sum = values.__class__.sum(values, axis=-1)
    values = values * diag_inner + values_sum.reshape(values_sum.shape + (1,))
    values = values.reshape(orig_shape)
    return values

# Given an evaluation point, compute the linear combination that computes
# that evaluation from evaluations on the hypercube. For example:
# chi_weights([3, 5]) gives [8, -12, -10, 15]. This means that if you have
# a 2D cube (ok fine, normies call that a square) [a, b, c, d], then that
# polynomial evaluated at (3, 5) equals 8*a - 12*b - 10*c + 15*d.
def chi_weights(coords, start_weights=None):
    if start_weights is None:
        weights = koalabear.ExtendedKoalaBear([[1,0,0,0]])
    else:
        weights = start_weights
    for c in coords:
        L = weights * (1 - c)
        R = weights * c
        weights = koalabear.ExtendedKoalaBear.append(L, R)
    return weights

# Given evaluations on a cube, compute the evaluation at the given coords
def mle_eval(cube, coords):
    for coord in coords:
        b = cube[::2]
        m = cube[1::2] - b
        cube = b + m * coord
    return cube[0]

# Given an evaluation point source_coords, take the cube that would be
# generated by chi_eval(source_coords), and evaluate it at eval_coords,
# without ever actually materializing that cube
def chi_eval(source_coords, eval_coords):
    o = 1
    for s, e in zip(source_coords, eval_coords):
        o *= (s * e + (1-s) * (1-e))
    return o

# Some more complicated methods, whose goal is to take the cube generated by
# chi_eval, and evaluate that cube *multiplied by M* at eval_coords
def compute_lower_order_weights(lower_coords):
    return koalabear.matmul(M.swapaxes(0,1), chi_weights(lower_coords))

def compute_weights(coords):
    intermediate = compute_lower_order_weights(coords[:4])
    return chi_weights(coords[4:], start_weights=intermediate)

def fast_point_eval(source_coords, eval_coords):
    intermediate = compute_lower_order_weights(source_coords[:4])
    o = mle_eval(intermediate, eval_coords[:4])
    o *= chi_eval(source_coords[4:], eval_coords[4:])
    return o

# Self-explanatory
def log2(x):
    return 0 if x <= 1 else 1 + log2(x//2)

# Given randomness, generates a random coord that is then used to generate
# the initial weights for GKR via chi_weights
def generate_weights_seed_coords(randomness, count, hash):
    return [
        koalabear.ExtendedKoalaBear(
            hash(randomness, koalabear.KoalaBear(31337+i)).value[:4]
        )
        for i in range(log2(count))
    ]

# Generate those weights
def generate_weights(randomness, count, hash):
    return chi_weights(generate_weights_seed_coords(randomness, count, hash))

# We'll make our implementation self-contained, and use our own hash in the prover
# and verifier!
def hash16_to_8(inp, permutation):
    return permutation(inp)[...,:8] + inp[...,:8]

def hash(*args, permutation):
    inputs = []
    for arg in args:
        inputs.append(koalabear.KoalaBear(arg.value.reshape((-1,))))
    buffer = koalabear.KoalaBear.append(*inputs)
    if SHA256_OVERRIDE:
        from hashlib import sha256
        d = sha256(buffer.tobytes()).digest()
        return koalabear.KoalaBear([
            int.from_bytes(d[i:i+4], 'little')
            for i in range(0,32,4)
        ])
    buffer_length = buffer.shape[0]
    # padding: buffer -> [length] + buffer + [pad to nearest 16]
    buffer = koalabear.KoalaBear.append(
        koalabear.KoalaBear([buffer_length]),
        buffer,
        koalabear.KoalaBear.zeros(15 - buffer.shape[0] % 16)
    )
    # Merkle tree style hash
    while buffer.shape[0] > 8:
        if buffer.shape[0] % 16 == 8:
            buffer = koalabear.KoalaBear.append(
                buffer,
                koalabear.KoalaBear.zeros(8)
            ) + buffer_length
            # Note: we mixin the total buffer length everywhere to avoid
            # collisions between inputs of different length
        buffer = buffer.reshape((-1, 16))
        buffer = hash16_to_8(buffer, permutation)
        buffer = buffer.reshape((-1,))
    return buffer
